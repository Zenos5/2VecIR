ost counter-examples found here have to do with the inherent ambiguities in defining the eigenvectors. There are essentially two kinds:

 - For repeating eigenvalues, the eigenvectors spanning their space are not uniuely defined. Only their subspace is. ny linearly indepedent basis for their space will do (extreme example : any full rank matrix  ualifies as matrix of right singular vectors. Likewise, any full rank matrix  can be chosen independently as a valid basis of left singular vectors).
 - If we exclude this by forcing distinct eigenvalues we still have each eigenvector only being uniue up to a scalar multiple, since any  satisfying   = \lambda  will also lead to \alpha  satisfying the same euation for any \alpha \ne 0. Forcing \left\|\right\|=1 eliminiates this only partially, in the real-valued case there is still a sign ambiguity (e.g., Roberts reply), in the complex-valued case a phase ambiguity.

These ambiguities aside, let us not forget that left singular vectors of  are right singular vectors of ^T. So if  has an EVD of the form = \Lambda ^{-1}, then ^T = ^{-T} \Lambda ^T.  This shows that  = ^{-1} does ualify as a pair of left/right singular vectors. It&#39;s only that if we compute  and  independently, we are not guaranteed to pick the pair of inverses among the set of ambiguities. If eigenvalues are distinct we might get a diagonal matrix (due to the scaling ambiguities), if they are not we might get something pretty arbitrary. 